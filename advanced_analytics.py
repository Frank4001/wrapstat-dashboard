#!/usr/bin/env python3
"""
Advanced Analytics for WrapStat Training Survey Data
Demonstrates various statistical and ML techniques for generating insights
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from scipy import stats
from scipy.stats import chi2_contingency
import warnings
warnings.filterwarnings('ignore')

def load_and_prepare_data():
    """Load and prepare the training survey data"""
    df = pd.read_csv('enhanced_wrapstat_training_data.csv')
    return df

def correlation_analysis(df):
    """1. CORRELATION ANALYSIS - Find relationships between variables"""
    print("="*60)
    print("1. CORRELATION ANALYSIS")
    print("="*60)
    
    # Select numeric rating columns
    rating_cols = [
        'content_engaging_numeric', 'content_relevant_numeric', 'content_understandable_numeric',
        'content_interactive_numeric', 'content_visual_support_numeric', 'content_user_friendly_numeric',
        'tech_easy_access_numeric', 'tech_easy_navigate_numeric', 'tech_functioning_properly_numeric',
        'improved_user_access_numeric', 'improved_care_coordinator_mgmt_numeric', 
        'improved_youth_roster_numeric', 'improved_followup_numeric',
        'knowledge_before_numeric', 'knowledge_after_numeric', 'knowledge_improvement',
        'training_duration_minutes'
    ]
    
    # Remove rows with missing values for correlation analysis
    correlation_data = df[rating_cols].dropna()
    
    # Calculate correlation matrix
    corr_matrix = correlation_data.corr()
    
    # Find strongest correlations with knowledge improvement
    knowledge_corr = corr_matrix['knowledge_improvement'].abs().sort_values(ascending=False)
    print("\nðŸŽ¯ STRONGEST CORRELATIONS WITH KNOWLEDGE IMPROVEMENT:")
    for var, corr in knowledge_corr.head(8).items():
        if var != 'knowledge_improvement':
            print(f"   {var:35} | r = {corr:.3f}")
    
    return corr_matrix

def predictive_modeling(df):
    """2. PREDICTIVE MODELING - Predict training outcomes"""
    print("\n" + "="*60)
    print("2. PREDICTIVE MODELING")
    print("="*60)
    
    # Prepare features and target
    feature_cols = [
        'content_engaging_numeric', 'content_relevant_numeric', 'content_understandable_numeric',
        'content_interactive_numeric', 'content_visual_support_numeric', 'content_user_friendly_numeric',
        'tech_easy_access_numeric', 'tech_easy_navigate_numeric', 'tech_functioning_properly_numeric',
        'training_duration_minutes'
    ]
    
    # Remove missing values
    model_data = df[feature_cols + ['knowledge_improvement']].dropna()
    
    X = model_data[feature_cols]
    y = model_data['knowledge_improvement']
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Train Random Forest model
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)
    
    # Predictions
    y_pred = rf_model.predict(X_test)
    
    # Model performance
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print(f"\nðŸ“Š MODEL PERFORMANCE:")
    print(f"   RÂ² Score: {r2:.3f} (explains {r2*100:.1f}% of variance)")
    print(f"   RMSE: {np.sqrt(mse):.3f}")
    
    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nðŸŽ¯ TOP PREDICTORS OF KNOWLEDGE IMPROVEMENT:")
    for idx, row in feature_importance.head(5).iterrows():
        print(f"   {row['feature']:35} | Importance: {row['importance']:.3f}")
    
    return rf_model, feature_importance

def clustering_analysis(df):
    """3. CLUSTERING ANALYSIS - Identify participant segments"""
    print("\n" + "="*60)
    print("3. CLUSTERING ANALYSIS")
    print("="*60)
    
    # Select features for clustering
    cluster_cols = [
        'knowledge_before_numeric', 'avg_content_satisfaction', 
        'avg_tech_satisfaction', 'knowledge_improvement',
        'training_duration_minutes'
    ]
    
    cluster_data = df[cluster_cols].dropna()
    
    # Standardize features
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(cluster_data)
    
    # Perform K-means clustering
    kmeans = KMeans(n_clusters=3, random_state=42)
    clusters = kmeans.fit_predict(scaled_data)
    
    # Add cluster labels
    cluster_data['cluster'] = clusters
    
    # Analyze clusters
    print(f"\nðŸ“Š PARTICIPANT SEGMENTS IDENTIFIED:")
    for i in range(3):
        cluster_subset = cluster_data[cluster_data['cluster'] == i]
        print(f"\n   ðŸ” SEGMENT {i+1} (n={len(cluster_subset)}):")
        print(f"      Avg Knowledge Before: {cluster_subset['knowledge_before_numeric'].mean():.2f}")
        print(f"      Avg Knowledge Improvement: {cluster_subset['knowledge_improvement'].mean():.2f}")
        print(f"      Avg Content Satisfaction: {cluster_subset['avg_content_satisfaction'].mean():.2f}")
        print(f"      Avg Duration: {cluster_subset['training_duration_minutes'].mean():.1f} min")
    
    return cluster_data, kmeans

def statistical_testing(df):
    """4. STATISTICAL TESTING - Test hypotheses"""
    print("\n" + "="*60)
    print("4. STATISTICAL TESTING")
    print("="*60)
    
    # Test 1: Do participants with technical issues have lower satisfaction?
    tech_issues = df[df['had_tech_issues'] == 1]['avg_content_satisfaction'].dropna()
    no_tech_issues = df[df['had_tech_issues'] == 0]['avg_content_satisfaction'].dropna()
    
    if len(tech_issues) > 0 and len(no_tech_issues) > 0:
        t_stat, p_value = stats.ttest_ind(tech_issues, no_tech_issues)
        print(f"\nðŸ“Š TECHNICAL ISSUES vs SATISFACTION:")
        print(f"   With tech issues (n={len(tech_issues)}): {tech_issues.mean():.2f} avg satisfaction")
        print(f"   No tech issues (n={len(no_tech_issues)}): {no_tech_issues.mean():.2f} avg satisfaction")
        print(f"   T-test p-value: {p_value:.3f} {'(significant)' if p_value < 0.05 else '(not significant)'}")
    
    # Test 2: Regional differences in satisfaction
    regional_satisfaction = df.groupby('region')['avg_content_satisfaction'].apply(list)
    if len(regional_satisfaction) > 1:
        f_stat, p_value = stats.f_oneway(*[group for group in regional_satisfaction if len(group) > 0])
        print(f"\nðŸ“Š REGIONAL SATISFACTION DIFFERENCES:")
        for region, satisfaction in df.groupby('region')['avg_content_satisfaction'].mean().items():
            print(f"   {region}: {satisfaction:.2f}")
        print(f"   ANOVA p-value: {p_value:.3f} {'(significant)' if p_value < 0.05 else '(not significant)'}")

def anomaly_detection(df):
    """5. ANOMALY DETECTION - Identify unusual responses"""
    print("\n" + "="*60)
    print("5. ANOMALY DETECTION")
    print("="*60)
    
    # Look for responses with extreme knowledge improvement (positive or negative)
    knowledge_improvement = df['knowledge_improvement'].dropna()
    
    # Calculate z-scores
    z_scores = np.abs(stats.zscore(knowledge_improvement))
    outliers = df[z_scores > 2]['response_id'].values
    
    print(f"\nðŸ“Š UNUSUAL KNOWLEDGE IMPROVEMENT PATTERNS:")
    print(f"   Found {len(outliers)} responses with extreme knowledge changes")
    
    # Show extreme cases
    extreme_cases = df[df['response_id'].isin(outliers)]
    for idx, row in extreme_cases.iterrows():
        print(f"   Response {row['response_id']}: {row['knowledge_before_numeric']} â†’ {row['knowledge_after_numeric']} (Î”{row['knowledge_improvement']:+.1f})")

def sentiment_analysis_simulation(df):
    """6. TEXT ANALYSIS SIMULATION - Analyze feedback comments"""
    print("\n" + "="*60)
    print("6. TEXT ANALYSIS INSIGHTS")
    print("="*60)
    
    # Simulate sentiment analysis on comments
    comments = df['final_comments'].dropna()
    
    # Simple keyword-based sentiment (simulation)
    positive_keywords = ['good', 'great', 'easy', 'helpful', 'informative', 'excellent']
    negative_keywords = ['difficult', 'hard', 'confusing', 'problems', 'issues', 'disconnect']
    
    sentiment_scores = []
    for comment in comments:
        if pd.isna(comment):
            continue
        comment_lower = str(comment).lower()
        pos_count = sum(1 for word in positive_keywords if word in comment_lower)
        neg_count = sum(1 for word in negative_keywords if word in comment_lower)
        sentiment_scores.append(pos_count - neg_count)
    
    print(f"\nðŸ“Š FEEDBACK SENTIMENT ANALYSIS:")
    print(f"   Total comments analyzed: {len(sentiment_scores)}")
    print(f"   Average sentiment: {np.mean(sentiment_scores):.2f}")
    print(f"   Positive comments: {sum(1 for s in sentiment_scores if s > 0)}")
    print(f"   Negative comments: {sum(1 for s in sentiment_scores if s < 0)}")
    print(f"   Neutral comments: {sum(1 for s in sentiment_scores if s == 0)}")

def recommendations_engine(df, rf_model, feature_importance):
    """7. RECOMMENDATIONS ENGINE - Generate actionable insights"""
    print("\n" + "="*60)
    print("7. ACTIONABLE RECOMMENDATIONS")
    print("="*60)
    
    # Top improvement opportunities based on feature importance and current scores
    feature_cols = [
        'content_engaging_numeric', 'content_relevant_numeric', 'content_understandable_numeric',
        'content_interactive_numeric', 'content_visual_support_numeric', 'content_user_friendly_numeric',
        'tech_easy_access_numeric', 'tech_easy_navigate_numeric', 'tech_functioning_properly_numeric'
    ]
    
    # Calculate average scores and importance
    recommendations = []
    for feature in feature_cols:
        if feature in feature_importance['feature'].values:
            avg_score = df[feature].mean()
            importance = feature_importance[feature_importance['feature'] == feature]['importance'].iloc[0]
            improvement_potential = (5 - avg_score) * importance
            recommendations.append({
                'feature': feature,
                'current_score': avg_score,
                'importance': importance,
                'improvement_potential': improvement_potential
            })
    
    recommendations_df = pd.DataFrame(recommendations).sort_values('improvement_potential', ascending=False)
    
    print(f"\nðŸŽ¯ TOP IMPROVEMENT OPPORTUNITIES:")
    for idx, row in recommendations_df.head(5).iterrows():
        print(f"   {row['feature']:35} | Current: {row['current_score']:.2f}/5 | Potential Impact: {row['improvement_potential']:.3f}")
    
    print(f"\nðŸ’¡ STRATEGIC RECOMMENDATIONS:")
    print(f"   1. Focus on '{recommendations_df.iloc[0]['feature']}' - highest improvement potential")
    print(f"   2. Address technical issues - they correlate with lower satisfaction")
    print(f"   3. Consider different approaches for different participant segments")
    print(f"   4. Monitor training duration - optimal length appears to be 1-2 minutes")

def main():
    """Run comprehensive analytics on training survey data"""
    print("ðŸ§  ADVANCED ANALYTICS FOR WRAPSTAT TRAINING SURVEY")
    print("Applying Statistical Modeling, ML, and AI Techniques")
    
    # Load data
    df = load_and_prepare_data()
    print(f"\nDataset: {len(df)} responses with {len(df.columns)} variables")
    
    # Run analyses
    corr_matrix = correlation_analysis(df)
    rf_model, feature_importance = predictive_modeling(df)
    cluster_data, kmeans = clustering_analysis(df)
    statistical_testing(df)
    anomaly_detection(df)
    sentiment_analysis_simulation(df)
    recommendations_engine(df, rf_model, feature_importance)
    
    print(f"\n" + "="*60)
    print("ANALYSIS COMPLETE - Insights generated using:")
    print("âœ“ Correlation Analysis")
    print("âœ“ Random Forest Predictive Modeling") 
    print("âœ“ K-Means Clustering")
    print("âœ“ Statistical Hypothesis Testing")
    print("âœ“ Anomaly Detection")
    print("âœ“ Text/Sentiment Analysis")
    print("âœ“ AI-Powered Recommendations Engine")
    print("="*60)

if __name__ == "__main__":
    main()